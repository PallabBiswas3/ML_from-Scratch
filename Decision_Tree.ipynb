{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b9aae15-4db1-4132-bfa3-1a1d23ed207f",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2a6192-2218-4af5-bf6d-e80ea7339458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PALLAB\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0              0.27         0.36            20.7      0.045   \n",
       "1               6.3              0.30         0.34             1.6      0.049   \n",
       "2               8.1              0.28         0.40             6.9      0.050   \n",
       "3               7.2              0.23         0.32             8.5      0.058   \n",
       "4               7.2              0.23         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         8.8        6  \n",
       "1         9.5        6  \n",
       "2        10.1        6  \n",
       "3         9.9        6  \n",
       "4         9.9        6  \n",
       "...       ...      ...  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"winequality-white.csv\", sep=\";\")  # replace with actual filename\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cd7994-a8f2-4acf-b23f-91666b2bbcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality\n",
       "6    2198\n",
       "5    1457\n",
       "7     880\n",
       "8     175\n",
       "4     163\n",
       "3      20\n",
       "9       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"quality\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b33780-4b24-444d-92cd-6a7661492482",
   "metadata": {},
   "source": [
    "# target transform: quality -> {0,1,2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "546b1cad-2f16-4518-bda7-50eccd86701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"winequality-white.csv\", sep=\";\")\n",
    "\n",
    "# 1a) \n",
    "def quality_to_class(q):\n",
    "    if q < 5:\n",
    "        return 0\n",
    "    elif q in (5, 6):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "df['quality'] = df['quality'].apply(quality_to_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a4c12-02f1-4cbd-80f1-7594f59a0ef8",
   "metadata": {},
   "source": [
    "# Z-score normalize all other attributes (exclude 'quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b674ba-eb2a-4730-9960-93f7f28dca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [c for c in df.columns if c != 'quality']\n",
    "df_z = df[features].apply(lambda col: (col - col.mean()) / (col.std(ddof=0) if col.std(ddof=0) != 0 else 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f299026d-6d7a-4f8b-99a5-a4e6c199f35e",
   "metadata": {},
   "source": [
    "# Discretize each normalized column into 4 equal-width bins indexed 0 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bffb8ba-a89c-4725-a161-b0e1cb55df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done. X shape: (4898, 11) class dist: [ 183 3655 1060]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def discretize_to_4_equal_width_bins(col):\n",
    "    mn, mx = col.min(), col.max()\n",
    "    if mx == mn:\n",
    "        return pd.Series(np.zeros(len(col), dtype=int), index=col.index)\n",
    "    width = (mx - mn) / 4.0\n",
    "    idx = np.floor((col - mn) / width).astype(int)\n",
    "    idx = np.clip(idx, 0, 3)\n",
    "    return pd.Series(idx.values, index=col.index)\n",
    "\n",
    "df_binned = pd.DataFrame({c: discretize_to_4_equal_width_bins(df_z[c]) for c in df_z.columns})\n",
    "df_binned['quality'] = df['quality'].values\n",
    "\n",
    "X = df_binned.drop('quality', axis=1).values.astype(int)\n",
    "y = df_binned['quality'].values.astype(int)\n",
    "\n",
    "print(\"Preprocessing done. X shape:\", X.shape, \"class dist:\", np.bincount(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07277f2-823b-463f-917f-8385cfb8f9cb",
   "metadata": {},
   "source": [
    "# Custom ID3-like Decision Tree (discrete attributes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c682997-114f-4951-875b-4fe5856e8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Node:\n",
    "    def __init__(self, attribute=None, children=None, label=None, samples_idx=None):\n",
    "        self.attribute = attribute\n",
    "        self.children = children or {}\n",
    "        self.label = label\n",
    "        self.samples_idx = samples_idx  \n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.label is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbddb11-706e-42d1-b89f-fdcc633fe5ae",
   "metadata": {},
   "source": [
    "# Entropy and Gini calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10c21426-88cf-4c76-9e54-29942df39da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(arr):\n",
    "    vals, counts = np.unique(arr, return_counts=True)\n",
    "    p = counts / counts.sum()\n",
    "    return -np.sum(p * np.log2(p + 1e-12))\n",
    "\n",
    "def gini(arr):\n",
    "    vals, counts = np.unique(arr, return_counts=True)\n",
    "    p = counts / counts.sum()\n",
    "    return 1.0 - np.sum(p**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4465a8-2b8b-4002-bf7e-f65dfc7238b7",
   "metadata": {},
   "source": [
    "#  Best attribute (col index) and its gain Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf49b989-0f26-4f88-8e14-aa7a50b42989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_attribute(X_all, y_all, samples_idx, candidate_attrs, criterion):\n",
    "    \n",
    "    X_sub = X_all[samples_idx]\n",
    "    y_sub = y_all[samples_idx]\n",
    "    if criterion == 'entropy':\n",
    "        base_imp = entropy(y_sub)\n",
    "    else:\n",
    "        base_imp = gini(y_sub)\n",
    "    best_gain = -np.inf\n",
    "    best_attr = None\n",
    "    for a in candidate_attrs:\n",
    "        vals, counts = np.unique(X_sub[:, a], return_counts=True)\n",
    "        weighted = 0.0\n",
    "        for v, c in zip(vals, counts):\n",
    "            idx = samples_idx[X_sub[:, a] == v]\n",
    "            yv = y_all[idx]\n",
    "            imp = entropy(yv) if criterion == 'entropy' else gini(yv)\n",
    "            weighted += (len(yv) / len(y_sub)) * imp\n",
    "        gain = base_imp - weighted\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_attr = a\n",
    "    return best_attr, best_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "687990bb-35f0-4552-b408-ddeb92b7ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(X_all, y_all, samples_idx=None, candidate_attrs=None, min_samples=10, criterion='entropy'):\n",
    "    if samples_idx is None:\n",
    "        samples_idx = np.arange(len(y_all))\n",
    "    if candidate_attrs is None:\n",
    "        candidate_attrs = list(range(X_all.shape[1]))\n",
    "\n",
    "    y_sub = y_all[samples_idx]\n",
    "    majority = Counter(y_sub).most_common(1)[0][0]\n",
    "\n",
    "    # stopping conditions\n",
    "    if len(y_sub) == 0:\n",
    "        return Node(label=majority, samples_idx=samples_idx)\n",
    "    if len(np.unique(y_sub)) == 1:\n",
    "        return Node(label=y_sub[0], samples_idx=samples_idx)\n",
    "    if len(y_sub) < min_samples:\n",
    "        return Node(label=majority, samples_idx=samples_idx)\n",
    "    if len(candidate_attrs) == 0:\n",
    "        return Node(label=majority, samples_idx=samples_idx)\n",
    "\n",
    "    attr, gain = best_attribute(X_all, y_all, samples_idx, candidate_attrs, criterion)\n",
    "    if attr is None:\n",
    "        return Node(label=majority, samples_idx=samples_idx)\n",
    "\n",
    "    node = Node(attribute=attr, samples_idx=samples_idx)\n",
    "    node.children = {}\n",
    "    X_sub = X_all[samples_idx]\n",
    "    for val in np.unique(X_sub[:, attr]):\n",
    "        child_idx = samples_idx[X_sub[:, attr] == val]\n",
    "        if child_idx.size == 0:\n",
    "            node.children[val] = Node(label=majority, samples_idx=child_idx)\n",
    "        else:\n",
    "            new_attrs = [a for a in candidate_attrs if a != attr]\n",
    "            node.children[val] = build_tree(X_all, y_all, samples_idx=child_idx, candidate_attrs=new_attrs, min_samples=min_samples, criterion=criterion)\n",
    "    return node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff343fcf-220e-4d37-8059-f1d67770147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(node, x):\n",
    "    while not node.is_leaf():\n",
    "        a = node.attribute\n",
    "        v = x[a]\n",
    "        if v in node.children:\n",
    "            node = node.children[v]\n",
    "        else:\n",
    "            # unseen value -> majority label among node.samples_idx\n",
    "            return Counter(y[node.samples_idx]).most_common(1)[0][0]\n",
    "    return node.label\n",
    "\n",
    "def predict_batch(node, X_input):\n",
    "    return np.array([predict_one(node, x) for x in X_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b0e226-bf55-4a5b-8e26-d3ed74a9fe93",
   "metadata": {},
   "source": [
    "# Reducing error Pruning (custom tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0453be6-995f-4824-a8aa-9a21d57c179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_nonleaf_nodes_postorder(node):\n",
    "    nodes = []\n",
    "    if node is None or node.is_leaf():\n",
    "        return nodes\n",
    "    for c in node.children.values():\n",
    "        nodes.extend(collect_nonleaf_nodes_postorder(c))\n",
    "    nodes.append(node)\n",
    "    return nodes\n",
    "\n",
    "def prune_custom_tree(root, X_val, y_val):\n",
    "    root = copy.deepcopy(root)\n",
    "    while True:\n",
    "        changed = False\n",
    "        nodes = collect_nonleaf_nodes_postorder(root)\n",
    "        for node in nodes:\n",
    "            # compute current accuracy on validation\n",
    "            y_pred_before = predict_batch(root, X_val)\n",
    "            acc_before = accuracy_score(y_val, y_pred_before)\n",
    "\n",
    "            # try converting node to leaf with majority label among node.samples_idx\n",
    "            maj = Counter(y[node.samples_idx]).most_common(1)[0][0]\n",
    "            backup_children = node.children\n",
    "            backup_attr = node.attribute\n",
    "            backup_label = node.label\n",
    "\n",
    "            node.children = {}\n",
    "            node.attribute = None\n",
    "            node.label = maj\n",
    "\n",
    "            y_pred_after = predict_batch(root, X_val)\n",
    "            acc_after = accuracy_score(y_val, y_pred_after)\n",
    "\n",
    "            if acc_after >= acc_before:\n",
    "                changed = True  # keep the pruning\n",
    "            else:\n",
    "                # revert\n",
    "                node.children = backup_children\n",
    "                node.attribute = backup_attr\n",
    "                node.label = backup_label\n",
    "        if not changed:\n",
    "            break\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113db9d4-5aab-48b0-96c9-fc2e40446169",
   "metadata": {},
   "source": [
    "# Reduced-error pruning for sklearn DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "513d3bc5-5d3e-4fcb-b744-7fca9ca74340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_node_sample_indices_sklearn(clf, X_train):\n",
    "    node_indicator = clf.decision_path(X_train)  # sparse matrix\n",
    "    n_nodes = clf.tree_.node_count\n",
    "    samples_per_node = [ [] for _ in range(n_nodes) ]\n",
    "    for i in range(X_train.shape[0]):\n",
    "        node_ids = node_indicator[i].nonzero()[1]\n",
    "        for nid in node_ids:\n",
    "            samples_per_node[nid].append(i)\n",
    "    return [np.array(li, dtype=int) for li in samples_per_node]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "342a936d-f95f-40c5-8df8-52fa5b5d5aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_sklearn_tree_reduced_error(clf, X_val, y_val, X_train_for_nodes, y_train_for_nodes):\n",
    "    # work on copy to be safe\n",
    "    clf = copy.deepcopy(clf)\n",
    "    tree = clf.tree_\n",
    "    children_left = tree.children_left\n",
    "    children_right = tree.children_right\n",
    "\n",
    "    # postorder nodes\n",
    "    postorder = []\n",
    "    def recurse(n):\n",
    "        if children_left[n] != -1:\n",
    "            recurse(children_left[n])\n",
    "        if children_right[n] != -1:\n",
    "            recurse(children_right[n])\n",
    "        postorder.append(n)\n",
    "    recurse(0)\n",
    "\n",
    "    # samples per node (indices into X_train_for_nodes)\n",
    "    samples_per_node = get_node_sample_indices_sklearn(clf, X_train_for_nodes)\n",
    "\n",
    "    changed_any = True\n",
    "    while True:\n",
    "        changed = False\n",
    "        for node in reversed(postorder):\n",
    "            # skip leaves\n",
    "            if children_left[node] == -1 and children_right[node] == -1:\n",
    "                continue\n",
    "\n",
    "            acc_before = accuracy_score(y_val, clf.predict(X_val))\n",
    "\n",
    "            # majority class for that node using y_train_for_nodes\n",
    "            idx = samples_per_node[node]\n",
    "            if idx.size == 0:\n",
    "                maj = Counter(y_train_for_nodes).most_common(1)[0][0]\n",
    "            else:\n",
    "                maj = Counter(y_train_for_nodes[idx]).most_common(1)[0][0]\n",
    "\n",
    "            # backup\n",
    "            left_b, right_b = children_left[node], children_right[node]\n",
    "            value_b = tree.value[node].copy()\n",
    "\n",
    "            # mutate to leaf\n",
    "            tree.children_left[node] = -1\n",
    "            tree.children_right[node] = -1\n",
    "            # set node.value to majority class counts (approx)\n",
    "            # tree.value shape (n_nodes, 1, n_classes)\n",
    "            class_idx = np.where(clf.classes_ == maj)[0][0]\n",
    "            tree.value[node][0, :] = 0.0\n",
    "            tree.value[node][0, class_idx] = 1.0\n",
    "\n",
    "            acc_after = accuracy_score(y_val, clf.predict(X_val))\n",
    "\n",
    "            if acc_after >= acc_before:\n",
    "                changed = True  # keep change\n",
    "            else:\n",
    "                # revert\n",
    "                tree.children_left[node] = left_b\n",
    "                tree.children_right[node] = right_b\n",
    "                tree.value[node] = value_b\n",
    "        if not changed:\n",
    "            break\n",
    "        changed_any = changed_any or changed\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0218ad40-3484-49a3-a63f-2f2bfd4745d1",
   "metadata": {},
   "source": [
    "# Using full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e56b5608-1bad-4cbd-9f21-748b1d169d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1/3 ---\n",
      "sizes -> train_main: (2612, 11) val: (653, 11) test: (1633, 11)\n",
      "ID3 entropy fold acc: 0.7672994488671158\n",
      "ID3 gini fold acc: 0.7685241886099204\n",
      "SK entropy fold acc: 0.7605633802816901\n",
      "SK gini fold acc: 0.7617881200244948\n",
      "\n",
      "--- Fold 2/3 ---\n",
      "sizes -> train_main: (2612, 11) val: (653, 11) test: (1633, 11)\n",
      "ID3 entropy fold acc: 0.7519902020820576\n",
      "ID3 gini fold acc: 0.7513778322106552\n",
      "SK entropy fold acc: 0.7372933251684017\n",
      "SK gini fold acc: 0.7372933251684017\n",
      "\n",
      "--- Fold 3/3 ---\n",
      "sizes -> train_main: (2612, 11) val: (654, 11) test: (1632, 11)\n",
      "ID3 entropy fold acc: 0.7720588235294118\n",
      "ID3 gini fold acc: 0.7757352941176471\n",
      "SK entropy fold acc: 0.7665441176470589\n",
      "SK gini fold acc: 0.7714460784313726\n",
      "\n",
      "Cross-Validation Results (mean across folds) \n",
      "ID3_entropy: Accuracy=0.7638, Macro-Precision=0.5039, Macro-Recall=0.3885\n",
      "ID3_gini: Accuracy=0.7652, Macro-Precision=0.4987, Macro-Recall=0.3939\n",
      "SK_entropy: Accuracy=0.7548, Macro-Precision=0.3788, Macro-Recall=0.3788\n",
      "SK_gini: Accuracy=0.7568, Macro-Precision=0.3862, Macro-Recall=0.3769\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import copy\n",
    "\n",
    "# Parameters \n",
    "MIN_SAMPLES_SPLIT = 10\n",
    "CV_FOLDS = 3\n",
    "PRUNING_VAL_RATIO = 0.2       # inside each CV fold: 80% train, 20% val for pruning\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "#  3-fold CV loop (train/prune/eval) \n",
    "kf = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "results = {\n",
    "    'ID3_entropy': {'acc': [], 'prec': [], 'rec': []},\n",
    "    'ID3_gini': {'acc': [], 'prec': [], 'rec': []},\n",
    "    'SK_entropy': {'acc': [], 'prec': [], 'rec': []},\n",
    "    'SK_gini': {'acc': [], 'prec': [], 'rec': []}\n",
    "}\n",
    "\n",
    "fold = 0\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    fold += 1\n",
    "    print(f\"\\n--- Fold {fold}/{CV_FOLDS} ---\")\n",
    "    X_train_full, X_test = X[train_idx], X[test_idx]\n",
    "    y_train_full, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # shuffle training before split for pruning\n",
    "    rng = np.random.RandomState(RANDOM_STATE + fold)\n",
    "    perm = rng.permutation(len(X_train_full))\n",
    "    X_train_full = X_train_full[perm]\n",
    "    y_train_full = y_train_full[perm]\n",
    "\n",
    "    # split train -> train_main (80%) and val (20%) for pruning\n",
    "    n_train_full = len(X_train_full)\n",
    "    split_i = int((1 - PRUNING_VAL_RATIO) * n_train_full)\n",
    "    X_tr_main = X_train_full[:split_i]\n",
    "    y_tr_main = y_train_full[:split_i]\n",
    "    X_val = X_train_full[split_i:]\n",
    "    y_val = y_train_full[split_i:]\n",
    "\n",
    "    print(\"sizes -> train_main:\", X_tr_main.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)\n",
    "\n",
    "    # Custom ID3 (entropy)\n",
    "    # build on the TRAIN MAIN portion; but build_tree expects samples_idx relative to full array X_all.\n",
    "    # To avoid complexity, we will pass X_all = X_tr_main and y_all = y_tr_main and use full indices there.\n",
    "    tree_ent = build_tree(X_tr_main, y_tr_main, samples_idx=np.arange(len(y_tr_main)), candidate_attrs=list(range(X.shape[1])), min_samples=MIN_SAMPLES_SPLIT, criterion='entropy')\n",
    "    tree_ent_pruned = prune_custom_tree(tree_ent, X_val, y_val)\n",
    "    y_pred_ent = predict_batch(tree_ent_pruned, X_test)\n",
    "    results['ID3_entropy']['acc'].append(accuracy_score(y_test, y_pred_ent))\n",
    "    results['ID3_entropy']['prec'].append(precision_score(y_test, y_pred_ent, average='macro', zero_division=0))\n",
    "    results['ID3_entropy']['rec'].append(recall_score(y_test, y_pred_ent, average='macro', zero_division=0))\n",
    "    print(\"ID3 entropy fold acc:\", results['ID3_entropy']['acc'][-1])\n",
    "\n",
    "    # Custom ID3 (gini)\n",
    "    tree_gini = build_tree(X_tr_main, y_tr_main, samples_idx=np.arange(len(y_tr_main)), candidate_attrs=list(range(X.shape[1])), min_samples=MIN_SAMPLES_SPLIT, criterion='gini')\n",
    "    tree_gini_pruned = prune_custom_tree(tree_gini, X_val, y_val)\n",
    "    y_pred_gini = predict_batch(tree_gini_pruned, X_test)\n",
    "    results['ID3_gini']['acc'].append(accuracy_score(y_test, y_pred_gini))\n",
    "    results['ID3_gini']['prec'].append(precision_score(y_test, y_pred_gini, average='macro', zero_division=0))\n",
    "    results['ID3_gini']['rec'].append(recall_score(y_test, y_pred_gini, average='macro', zero_division=0))\n",
    "    print(\"ID3 gini fold acc:\", results['ID3_gini']['acc'][-1])\n",
    "\n",
    "    # sklearn DecisionTree (entropy) \n",
    "    clf_ent = DecisionTreeClassifier(criterion='entropy', min_samples_split=MIN_SAMPLES_SPLIT, random_state=RANDOM_STATE)\n",
    "    clf_ent.fit(X_tr_main, y_tr_main)\n",
    "    clf_ent_pruned = prune_sklearn_tree_reduced_error(clf_ent, X_val, y_val, X_tr_main, y_tr_main)\n",
    "    y_pred_sk_ent = clf_ent_pruned.predict(X_test)\n",
    "    results['SK_entropy']['acc'].append(accuracy_score(y_test, y_pred_sk_ent))\n",
    "    results['SK_entropy']['prec'].append(precision_score(y_test, y_pred_sk_ent, average='macro', zero_division=0))\n",
    "    results['SK_entropy']['rec'].append(recall_score(y_test, y_pred_sk_ent, average='macro', zero_division=0))\n",
    "    print(\"SK entropy fold acc:\", results['SK_entropy']['acc'][-1])\n",
    "\n",
    "    # sklearn DecisionTree (gini) \n",
    "    clf_g = DecisionTreeClassifier(criterion='gini', min_samples_split=MIN_SAMPLES_SPLIT, random_state=RANDOM_STATE)\n",
    "    clf_g.fit(X_tr_main, y_tr_main)\n",
    "    clf_g_pruned = prune_sklearn_tree_reduced_error(clf_g, X_val, y_val, X_tr_main, y_tr_main)\n",
    "    y_pred_sk_g = clf_g_pruned.predict(X_test)\n",
    "    results['SK_gini']['acc'].append(accuracy_score(y_test, y_pred_sk_g))\n",
    "    results['SK_gini']['prec'].append(precision_score(y_test, y_pred_sk_g, average='macro', zero_division=0))\n",
    "    results['SK_gini']['rec'].append(recall_score(y_test, y_pred_sk_g, average='macro', zero_division=0))\n",
    "    print(\"SK gini fold acc:\", results['SK_gini']['acc'][-1])\n",
    "\n",
    "\n",
    "print(\"\\nCross-Validation Results (mean across folds) \")\n",
    "for key in results:\n",
    "    print(f\"{key}: Accuracy={np.mean(results[key]['acc']):.4f}, Macro-Precision={np.mean(results[key]['prec']):.4f}, Macro-Recall={np.mean(results[key]['rec']):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
